job_id_regexp: "Job Id:(\\d+)"
cmd: "sbatch {sbatch_script}"
check_interval_secs: 600 # 10 minutes before running start_condition_cmd again
PARTITION: booster
ACCOUNT: projectnucleus
TIME: 360
NODES: 1
LOGS: "/p/data1/mmlaion/porian1/megatron_ref_logs/scaling_exps"
PRETRAINED: ""
ENVIRONMENT:
  MASTER_ADDR: $(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)

PATHS:
  SHARED_CONTAINERS: "/p/data1/mmlaion/shared/containers"
  IMAGE_NAME: "pytorch_24.09-py3.sif"
  SHARED_MEGATRON: "/p/data1/mmlaion/shared/repos/Megatron-LM"
  PROJECT_DIR_PATH: "/p/data1/mmlaion/shared"

# # Checkpoint conversion configuration 
CONVERSION:
  OPEN_SCI_HF_PATH: "/p/project1/ccstdl/porian1/Megatron-LM-Open-Sci/Open-Sci-hf"
  ARCHITECTURE: "OpensciForCausalLM" 
  MODEL_TYPE: "opensci"
  TOKENIZER_NAME: "EleutherAI/gpt-neox-20b"
  INTERMEDIATE_SIZE: "expr({HIDDEN_SIZE}*4)"  # Standard 4x expansion
  VOCAB_SIZE: 50304  # Fixed vocab size for the tokenizer
  TARGET_TP_SIZE: 1
  TARGET_PP_SIZE: 1
  TASKS_PATH: "/p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/configs/oellm-core"
  HF_HOME: "/p/data1/mmlaion/porian1/.cache/huggingface"

EXPERIMENTS:
  - 10M:
      NUM_LAYERS: 5
      HIDDEN_SIZE: 160
      NUM_ATTN_HEADS: 4
      FFN_HIDDEN_SIZE: 640
      MODEL: "10M"
  - 25M:
      NUM_LAYERS: 9
      HIDDEN_SIZE: 288
      NUM_ATTN_HEADS: 4
      FFN_HIDDEN_SIZE: 1152
      MODEL: "25M"
  - 50M:
      NUM_LAYERS: 12
      HIDDEN_SIZE: 384
      NUM_ATTN_HEADS: 6
      FFN_HIDDEN_SIZE: 1536
      MODEL: "50M"
  # - 100M:
  #     NUM_LAYERS: 15
  #     HIDDEN_SIZE: 512
  #     NUM_ATTN_HEADS: 8
  #     FFN_HIDDEN_SIZE: 2048
  #     MODEL: "100M"
  # - 140M:
  #     NUM_LAYERS: 22
  #     HIDDEN_SIZE: 512
  #     NUM_ATTN_HEADS: 8 
  #     FFN_HIDDEN_SIZE: 2256
  #     MODEL: "140M"
  # - 400M:
  #     NUM_LAYERS: 22
  #     HIDDEN_SIZE: 1024
  #     NUM_ATTN_HEADS: 16
  #     FFN_HIDDEN_SIZE: 3840
  #     MODEL: "400M"
SEQ_LENGTH: 2048 
MAX_POSITION_EMBEDDINGS: 2048
BETA1: 0.9
BETA2: 0.95
CLIP_GRAD: 1.0
LR: [1e-2, 5e-3, 1e-3, 5e-4, 1e-4] # LR for each experiment
WD: "expr(1e-4 / {LR})" # LR*WD = 1e-4 always
MIN_LR: 0.0
TOTAL_TOKENS_NUM: 2_000_000_000 # x20 is 200M, so this is 5x CC
MICRO_BATCH_SIZE: [4,16,32]
GAS: 1

BIAS: 
  - WITH:
      DISABLE_BIAS_LINEAR: ""  # LLaMA-style architecture without bias
      BIAS_NAME: "with"
  - WITHOUT:
      DISABLE_BIAS_LINEAR: "--disable-bias-linear"  # LLaMA-style
      BIAS_NAME: "without"
  # ["--disable-bias-linear", ""]  # LLaMA-style architecture without bias
LR_WARMUP_ITERS: "expr(min(2*(12*({HIDDEN_SIZE}**2)*{NUM_LAYERS} + {HIDDEN_SIZE}*{VOCAB_SIZE}) // ({SEQ_LENGTH} * {GLOBAL_BATCH_SIZE}), 5000))" 
TP: 1
PP: 1

# computed args 
NUM_GPUS: "expr({NODES}*4)"
GLOBAL_BATCH_SIZE: "expr(int(({NUM_GPUS} * {MICRO_BATCH_SIZE} * {GAS})/{TP}))"
TRAIN_ITERS: "expr(({TOTAL_TOKENS_NUM} + {SEQ_LENGTH} * {GLOBAL_BATCH_SIZE} - 1) // ({SEQ_LENGTH} * {GLOBAL_BATCH_SIZE}))"
LR_DECAY_ITERS: "expr({TRAIN_ITERS} - {LR_WARMUP_ITERS})"
LR_WSD_DECAY_STYLE: linear # to be removed from here and moved to the phase, template change needed. If running cosine it should be cosine here?

#logging
SAVE_INTERVAL: 2000 # for real runs should be 2000 or so, also depends on cooldown budget(s)  
LOG_INTERVAL: 100 # for real runs should be 100 or so
EVAL_ITERS: 1
PHASE:
    - PRETRAIN:
        RUN_DIR: "{LOGS}/{EXP_NAME}"
        LR_DECAY_STYLE: WSD # Default is cosine
        # EPOCHS: "{max_epochs}"
        PHASE_PARAMS: ""
        RESUME: latest
        COOLDOWN_FRACTION: 0
        LR_WSD_DECAY_ITERS: "expr({TRAIN_ITERS}*{COOLDOWN_FRACTION})" # placeholder, to be removed
        MODE:
            - TRAIN:
                template: /p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/configs/train_container_simple.sbatch
                sbatch_script: "/p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/sbatch_scripts/scaling_exps/{LR_DECAY_STYLE}_lr/{EXP_NAME}_train.sbatch"
                output_file: "{LOGS}/{EXP_NAME}/slurm_train.out"
                termination_cmd: 'train_count=$(grep -E "after training is done|KeyError" {LOGS}/{EXP_NAME}/slurm_train.out 2>/dev/null|wc -l); echo $(( train_count > 0 ? 1 : 0 ))'
            - CONVERT_EVAL:
                template: /p/project1/ccstdl/porian1/Megatron-LM-Open-Sci/configs/convert_eval_checkpoint.sbatch
                sbatch_script: "/p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/sbatch_scripts/scaling_exps/{LR_DECAY_STYLE}_lr/{EXP_NAME}_convert.sbatch"
                output_file: "{LOGS}/{EXP_NAME}/slurm_convert.out"
                start_condition_cmd: "/p/project1/ccstdl/porian1/Megatron-LM-Open-Sci/configs/convert_helper.sh {RUN_DIR}"
                termination_cmd: 'convert_done=$(/p/project1/ccstdl/porian1/Megatron-LM-Open-Sci/configs/convert_helper.sh {RUN_DIR}); train_count=$(grep -E "after training is done|KeyError" {LOGS}/{EXP_NAME}/slurm_train.out 2>/dev/null|wc -l); if [ $train_count -gt 0 ]; then train_done=1; else train_done=0; fi; echo $(( (1 - convert_done) * train_done ))'
                TARGET_HF_PATH: "{LOGS}/{EXP_NAME}/converted_hf"
                SOURCE_CHECKPOINT_PATH: "{LOGS}/{EXP_NAME}/checkpoints"
    - COOLDOWN:
        RUN_DIR: "{LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}"
        LR_DECAY_STYLE: WSD
        LR_WSD_DECAY_ITERS: "expr(int({TRAIN_ITERS}*{COOLDOWN_FRACTION}))"
        TRAIN_ITERS: "expr(({TOTAL_TOKENS_THIS_PHASE} + {SEQ_LENGTH} * {GLOBAL_BATCH_SIZE} - 1) // ({SEQ_LENGTH} * {GLOBAL_BATCH_SIZE}))"
        ITER: "expr(int(({TRAIN_ITERS} - {LR_WSD_DECAY_ITERS}) / {SAVE_INTERVAL}) * {SAVE_INTERVAL})" 
        COOLDOWN_FRACTION: 0.1
        TOKENS_NUM: "expr(int({TOTAL_TOKENS_NUM}*0.1))"
        LR_DECAY_ITERS: "{TRAIN_ITERS}"  # Add this line
        CD_SCALE: # determine by % of total tokens
            - 100M:
                TOTAL_TOKENS_THIS_PHASE: 100_000_000
            - 200M:
                TOTAL_TOKENS_THIS_PHASE: 200_000_000
            - 400M:
                TOTAL_TOKENS_THIS_PHASE: 400_000_000
            - 1B:
                TOTAL_TOKENS_THIS_PHASE: 1_000_000_000
            - 2B:
                TOTAL_TOKENS_THIS_PHASE: 2_000_000_000
        PHASE_PARAMS: "{CD_SCALE}"
        MODE:
            - TRAIN:
                template: /p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/configs/train_container_simple.sbatch 
                sbatch_script: "/p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/sbatch_scripts/scaling_exps/{LR_DECAY_STYLE}_lr/{EXP_NAME}_cooldown_s{CD_SCALE}_train.sbatch"
                output_file: "{LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/slurm_train.out"
                start_condition_cmd: "/p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/configs/cooldown_helper.sh {LOGS} {EXP_NAME} {ITER} {CD_SCALE}"
                termination_cmd: 'train_count=$(grep "after training is done" {LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/slurm_train.out 2>/dev/null|wc -l); echo $(( train_count > 0 ? 1 : 0 ))'
            - CONVERT_EVAL:
                template: /p/project1/ccstdl/porian1/Megatron-LM-Open-Sci/configs/convert_eval_checkpoint.sbatch
                sbatch_script: "/p/home/jusers/porian1/juwels/porian1/Megatron-LM-Open-Sci/sbatch_scripts/scaling_exps/{LR_DECAY_STYLE}_lr/{EXP_NAME}_cooldown_s{CD_SCALE}_convert.sbatch"
                output_file: "{LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/slurm_convert.out"
                start_condition_cmd: "/p/project1/ccstdl/porian1/Megatron-LM-Open-Sci/configs/convert_helper.sh {RUN_DIR}"
                termination_cmd: 'convert_done=$(/p/project1/ccstdl/porian1/Megatron-LM-Open-Sci/configs/convert_helper.sh {RUN_DIR}); train_count=$(grep "after training is done" {LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/slurm_train.out 2>/dev/null|wc -l); if [ $train_count -gt 0 ]; then train_done=1; else train_done=0; fi; echo $(( (1 - convert_done) * train_done ))'
                TARGET_HF_PATH: "{LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/converted_hf"
                SOURCE_CHECKPOINT_PATH: "{LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/checkpoints"
DATASET: 
    - C4:
        DATA_PATH: "/p/data1/mmlaion/text-data/tokenized/c4/merged"
        TOKENIZER_MODEL: "/p/data1/mmlaion/marianna/models/EleutherAI/gpt-neox-20b"
        TOKENIZER_TYPE: "HuggingFaceTokenizer"
        DATA_NUM_WORKERS: 8
        DATASET_NAME: "c4"
    
EXP_NAME: "{DATASET_NAME}_{MODEL}_lr{LR}_b1_{BETA1}_b2_{BETA2}_wd{WD}_w{LR_WARMUP_ITERS}_n{NODES}_bs{MICRO_BATCH_SIZE}__{BIAS_NAME}Bias"
name: "{EXP_NAME}_{PHASE}{PHASE_PARAMS}_{MODE}"
