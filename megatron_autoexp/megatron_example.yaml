job_id_regexp: "Job Id:(\\d+)"
cmd: "sbatch {sbatch_script}"
check_interval_secs: 60 # 10 minutes before running start_condition_cmd again
PARTITION: develbooster
ACCOUNT: transfernetx
TIME: 20
NODES: 1
LOGS: "PATH_TO_LOGS_DIR"
PRETRAINED: ""
ENVIRONMENT:
  MASTER_ADDR: $(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)

PATHS:
  SHARED_CONTAINERS: "/p/data1/mmlaion/shared/containers"

EXPERIMENTS:
  SMALL:
    MODEL_SCALE:
        ARCH_ARGS: # 1.3B
            NUM_LAYERS: 2
            HIDDEN_SIZE: 2048
            NUM_ATTN_HEADS: 16 # Default for this size is 16
            MAX_POSITION_EMBEDDINGS: 2048 # Default is 1024
            SEQ_LENGTH: 2048 # Default is 1024
            MODEL: "1.3B"
        WD: 0.05
        BETA1: 0.9
        BETA2: 0.95
        CLIP_GRAD: 1.0
        LR: 1e-3
        MIN_LR: 0.0
        
    MICRO_BATCH_SIZE: 24
    GAS: 1
    TOTAL_TOKENS_NUM: 50_000_000 # 50M tokens
    # LR_WARMUP_ITERS: 5000 # tested against 2000
    LR_WARMUP_ITERS: 7 # for testing
    TP: 1
    PP: 1

    # computed args 
    NUM_GPUS: "expr({NODES}*4)"
    GLOBAL_BATCH_SIZE: "expr(int(({NUM_GPUS} * {MICRO_BATCH_SIZE} * {GAS})/{TP}))"
    TRAIN_ITERS: "expr(({TOTAL_TOKENS_NUM} + {SEQ_LENGTH} * {GLOBAL_BATCH_SIZE} - 1) // ({SEQ_LENGTH} * {GLOBAL_BATCH_SIZE}))"
    LR_DECAY_ITERS: "expr({TRAIN_ITERS} - {LR_WARMUP_ITERS})"
    LR_WSD_DECAY_STYLE: linear # to be removed from here and moved to the phase, template change needed. If running cosine it should be cosine here?

    #logging
    SAVE_INTERVAL: 100 # for real runs should be 2000 or so, also depends on cooldown budget(s)  
    LOG_INTERVAL: 10 # for real runs should be 100 or so
    EVAL_ITERS: 1
PHASE:
    - PRETRAIN:
        RUN_DIR: "{LOGS}/{EXP_NAME}"
        LR_DECAY_STYLE: WSD # Default is cosine
        # EPOCHS: "{max_epochs}"
        PHASE_PARAMS: ""
        RESUME: latest
        COOLDOWN_FRACTION: 0
        LR_WSD_DECAY_ITERS: "expr({TRAIN_ITERS}*{COOLDOWN_FRACTION})" # placeholder, to be removed
        MODE:
            - TRAIN:
                template: train_container_simple.sbatch # Edit this to full path
                sbatch_script: "PATH_TO_SBATCH_SCRIPT_DIR/{LR_DECAY_STYLE}_lr/{EXP_NAME}_train.sbatch"
                output_file: "{LOGS}/{EXP_NAME}/slurm_train.out"
                termination_cmd: 'grep -E "after training is done|KeyError" {LOGS}/{EXP_NAME}/slurm_train.out|wc -l'
            # - EVAL: TODO!
    - COOLDOWN:
        RUN_DIR: "{LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}"
        LR_DECAY_STYLE: WSD
        LR_WSD_DECAY_ITERS: "expr(int({TRAIN_ITERS}*{COOLDOWN_FRACTION}))"
        ITER: "expr(int(({TRAIN_ITERS} - {LR_WSD_DECAY_ITERS}) / {SAVE_INTERVAL}) * {SAVE_INTERVAL})" 
        COOLDOWN_FRACTION: "expr({TOKENS_NUM}/{TOTAL_TOKENS_NUM})"
        CD_SCALE:
            - 20M:
                TOKENS_NUM: 20_000_000
            - 10M:
                TOKENS_NUM: 10_000_000
            # - 5M:
            #     TOKENS_NUM: 5_000_000
        PHASE_PARAMS: "{CD_SCALE}"
        MODE:
            - TRAIN:
                template: train_container_simple.sbatch # Edit this to full path
                sbatch_script: "PATH_TO_SBATCH_SCRIPT_DIR/{LR_DECAY_STYLE}_lr/{EXP_NAME}_cooldown_s{CD_SCALE}_train.sbatch"
                output_file: "{LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/slurm_train.out"
                start_condition_cmd: "PATH_TO_CD_HELPER_SCRIPT/cooldown_helper.sh {LOGS} {EXP_NAME} {ITER} {CD_SCALE}"
                termination_cmd: 'grep "after training is done" {LOGS}/{EXP_NAME}/cooldown_s{CD_SCALE}/slurm_train.out|wc -l'
                # resume: "$(if [ -f '{LOGS}/{FOLDER_NAME}/checkpoints/epoch_latest.pt' ]; then echo 'latest'; else echo '{checkpoint}'; fi)"
            # - EVAL: TODO!
DATASET: 
    - C4:
        DATA_PATH: "/p/data1/mmlaion/text-data/tokenized/c4/merged"
        TOKENIZER_MODEL: "/p/data1/mmlaion/marianna/models/EleutherAI/gpt-neox-20b"
        TOKENIZER_TYPE: "HuggingFaceTokenizer"
        DATA_NUM_WORKERS: 8
        DATASET_NAME: "c4"
    
EXP_NAME: "{DATASET_NAME}_{MODEL}_lr{LR}_b1_{BETA1}_b2_{BETA2}_wd{WD}_w{LR_WARMUP_ITERS}_n{NODES}_bs{MICRO_BATCH_SIZE}"
name: "{EXP_NAME}_{PHASE}{PHASE_PARAMS}_{MODE}"
